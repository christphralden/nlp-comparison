{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2157,"sourceType":"datasetVersion","datasetId":18}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn torch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n# most likely u dont need this\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertPreTrainedModel\nfrom torch import nn\n\nclass BERTCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BERTCNN, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=128, kernel_size=5, padding=2)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(0.5)\n        self.classifier = nn.Linear(128, config.num_labels)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n        sequence_output = outputs[0]\n        sequence_output = sequence_output.permute(0, 2, 1)\n        x = self.conv(sequence_output)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n\n        return (loss, logits) if loss is not None else logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertModel, BertPreTrainedModel, BertConfig, AutoTokenizer, TrainingArguments, Trainer\n\nclass TransBLSTM(BertPreTrainedModel):\n    def __init__(self, config):\n        super(TransBLSTM, self).__init__(config)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=config)\n        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, \n                             num_layers=1, bidirectional=True, batch_first=True)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.dropout = nn.Dropout(0.5)\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, \n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n        bert_outputs = self.bert(input_ids, attention_mask=attention_mask, \n                                 token_type_ids=token_type_ids, position_ids=position_ids, \n                                 head_mask=head_mask, inputs_embeds=inputs_embeds)\n        \n        sequence_output = bert_outputs[0]\n        blstm_output, _ = self.blstm(sequence_output)\n        combined_output = self.layer_norm(sequence_output + blstm_output)\n        \n        pooled_output = combined_output[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n        return (loss, logits) if loss is not None else logits\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nclass RoBERTa:\n    def __init__(self, model_type='cardiffnlp/twitter-roberta-base-sentiment', num_labels=3):\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=num_labels)\n\n    def get_model(self):\n        return self.model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training & Data Cleanup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, BertConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom datasets import Dataset\nimport torch\n\ndef load_and_balance_data(file_path, target_count=10000):\n    df = pd.read_csv(file_path, on_bad_lines='skip', nrows=100000)\n    df['Sentiment'] = df['Score'].apply(map_score_to_sentiment)\n    \n    # Ensure samples\n    def ensure_samples(data, target_count):\n        if len(data) >= target_count:\n            return data.sample(target_count, random_state=42)\n        else:\n            return data.sample(target_count, replace=True, random_state=42)\n    \n    negative_df = ensure_samples(df[df['Sentiment'] == 0], target_count)\n    neutral_df = ensure_samples(df[df['Sentiment'] == 1], target_count)\n    positive_df = ensure_samples(df[df['Sentiment'] == 2], target_count)\n    \n    balanced_df = pd.concat([negative_df, neutral_df, positive_df]).sample(frac=1, random_state=42)  # Shuffle dataset\n    \n    train_df, temp_df = train_test_split(balanced_df[['Text', 'Sentiment']], test_size=0.3, random_state=42)\n    test_df, val_df = train_test_split(temp_df, test_size=1/3, random_state=42)\n    \n    return train_df, test_df, val_df\n\ndef map_score_to_sentiment(score):\n    return 0 if score < 3 else (1 if score == 3 else 2)\n\ndef save_to_csv(data_df, file_name):\n    data_df.to_csv(file_name, index=False)\n\ndef tokenize_data(tokenizer, texts, labels):\n    tokenized_inputs = tokenizer(texts.tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    return Dataset.from_dict({**tokenized_inputs, 'labels': labels.tolist()})\n\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"f1\": f1_score(p.label_ids, preds, average='macro')}\n\ndef train_model(model, train_dataset, val_dataset, tokenizer, output_dir):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        report_to=\"none\",\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=64,\n        warmup_steps=500,\n        weight_decay=0.01,\n        logging_dir='./logs',\n        logging_steps=10,\n        save_total_limit=2,\n        save_steps=500,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\" \n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n\n    model_path = f\"{output_dir}/best_model\"\n    model.save_pretrained(model_path)\n    tokenizer.save_pretrained(model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df, val_df = load_and_balance_data('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')\n\nsave_to_csv(train_df, '/kaggle/working/balanced_train_data.csv')\nsave_to_csv(test_df, '/kaggle/working/balanced_test_data.csv')\nsave_to_csv(val_df, '/kaggle/working/balanced_val_data.csv')\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntrain_dataset = tokenize_data(tokenizer, train_df['Text'], train_df['Sentiment'])\ntest_dataset = tokenize_data(tokenizer, test_df['Text'], test_df['Sentiment'])\nval_dataset = tokenize_data(tokenizer, val_df['Text'], val_df['Sentiment'])\n\n\ndef data_stats(data_df):\n    num_reviews = data_df.shape[0]\n    print(f\"Total number of reviews: {num_reviews}\")\n\n    lengths = data_df['Text'].apply(len)\n    average_length = lengths.mean()\n    print(f\"Average review length: {average_length:.2f} characters\")\n\n    all_words = ' '.join(data_df['Text']).split()\n    vocab_size = len(set(all_words))\n    print(f\"Vocabulary size: {vocab_size}\")\n\n    median_length = lengths.median()\n    min_length = lengths.min()\n    max_length = lengths.max()\n    print(f\"Median review length: {median_length} characters\")\n    print(f\"Minimum review length: {min_length} characters\")\n    print(f\"Maximum review length: {max_length} characters\")\n\n    negative_count = (data_df['Sentiment'] == 0).sum()\n    neutral_count = (data_df['Sentiment'] == 1).sum()\n    positive_count = (data_df['Sentiment'] == 2).sum()\n    print(f'Negative Count: {negative_count}')\n    print(f'Neutral Count: {neutral_count}')\n    print(f'Positive Count: {positive_count}')\n\nprint(\"Training Data Statistics:\")\ndata_stats(train_df)\nprint(\"\\nTesting Data Statistics:\")\ndata_stats(test_df)\nprint(\"Validation Data Statistics:\")\ndata_stats(val_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"bert_model_type = 'bert-base-uncased'\nbert_cnn_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\nbert_cnn_model = BERTCNN(config=bert_cnn_config)\ntrain_model(bert_cnn_model, train_dataset, val_dataset, tokenizer, './bert_cnn_results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_type = \"bert-base-uncased\"\ntrans_blstm_config = BertConfig.from_pretrained(bert_model_type, num_labels=3)\ntrans_blstm_model =  TransBLSTM.from_pretrained(bert_model_type, config=trans_blstm_config)\ntrain_model(trans_blstm_model, train_dataset, val_dataset, tokenizer, \"./trans_blstm_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_model_type = 'cardiffnlp/twitter-roberta-base-sentiment'\nroberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_type)\nroberta_train_dataset = tokenize_data(roberta_tokenizer,  train_df['Text'], train_df['Sentiment'])\nroberta_test_dataset = tokenize_data(roberta_tokenizer,  test_df['Text'], test_df['Sentiment'])\nroberta_val_dataset = tokenize_data(roberta_tokenizer, val_df['Text'], val_df['Sentiment'])\n\nroberta_model = RoBERTa(model_type=roberta_model_type).get_model()\ntrain_model(roberta_model, roberta_train_dataset, roberta_test_dataset, roberta_val_dataset, './roberta_results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optionally Load Models","metadata":{}},{"cell_type":"code","source":"# model_dir = \"/kaggle/working/bert_cnn_results/best_model/\"\n\n# loaded_bert_cnn_config = BertConfig.from_pretrained(model_dir)\n# loaded_bert_cnn_model = BERTCNN.from_pretrained(model_dir, config=config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download","metadata":{}},{"cell_type":"code","source":"# !zip -r roberta_results.zip /kaggle/working/roberta_results/best_model/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cd roberta_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'roberta_results.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_dataset):\n    trainer = Trainer(model=model)\n    result = trainer.predict(test_dataset)\n    prediction = np.argmax(result.predictions, axis=1)\n    return result, prediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert_cnn_result, bert_cnn_preds = test_model(bert_cnn_model, test_dataset) # from runtime\nbert_cnn_result, bert_cnn_preds = test_model(loaded_bert_cnn_model, test_dataset) # from directory","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_blstm_result, trans_blstm_preds = test_model(trans_blstm_model, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_result, roberta_preds = test_model(roberta_model, roberta_test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"def calculate_f1(tp, fp, fn):\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    if (precision + recall) == 0:\n        return 0\n    return 2 * (precision * recall) / (precision + recall)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare_deprecated(model_result, model_preds, model_type):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    falsy_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['Sentiment'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n        if score > 0 and preds == 0:\n            falsy_map[cases[0]].append(i)\n        elif score != 1 and preds == 1:\n            falsy_map[cases[1]].append(i)\n        elif score <2 and preds == 2:\n            falsy_map[cases[2]].append(i)\n\n\n    total_data = len(predictions_map[cases[0]]) + len(predictions_map[cases[1]]) + len(predictions_map[cases[2]])\n\n    print(\"Predictions\")\n    print(f'Negative:{len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative:{len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"False Positives\")\n    print(f'Negative:{len(falsy_map[cases[0]])} ({len(falsy_map[cases[0]])/len(truth_map[cases[0]])*100})| Neutral: {len(falsy_map[cases[1]])} ({len(falsy_map[cases[1]])/len(truth_map[cases[1]])*100})| Positive: {len(falsy_map[cases[2]])} ({len(falsy_map[cases[2]])/len(truth_map[cases[2]])*100})')\n    \n\n    tp_negative = len(predictions_map[cases[0]]) - len(falsy_map[cases[0]])\n    tp_neutral = len(predictions_map[cases[1]]) - len(falsy_map[cases[1]])\n    tp_positive = len(predictions_map[cases[2]]) - len(falsy_map[cases[2]])\n    \n    fn_negative = len(truth_map[cases[0]]) - tp_negative\n    fn_neutral = len(truth_map[cases[1]]) - tp_neutral\n    fn_positive = len(truth_map[cases[2]]) - tp_positive\n    \n    \n    f1_negative = calculate_f1(tp_negative, len(falsy_map[cases[0]]), fn_negative)\n    f1_neutral = calculate_f1(tp_neutral, len(falsy_map[cases[1]]), fn_neutral)\n    f1_positive = calculate_f1(tp_positive, len(falsy_map[cases[2]]), fn_positive)\n    average_f1 = (f1_negative + f1_neutral + f1_positive) / 3\n\n    print(f\"F1 Score Negative: {f1_negative:.4f}\")\n    print(f\"F1 Score Neutral: {f1_neutral:.4f}\")\n    print(f\"F1 Score Positive: {f1_positive:.4f}\")\n    print(f\"Average F1 Score: {average_f1:.4f}\")\n    print(\"============\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compare(model_result, model_preds, model_type, test_df):\n    print(model_type)\n    cases = ['negative', 'neutral', 'positive']\n\n    predictions_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n    truth_map = {\n        'negative': [],\n        'neutral': [],\n        'positive': [],\n    }\n\n    for i, (result, preds) in enumerate(zip(model_result, model_preds)):\n        score = test_df['Sentiment'].iloc[i]\n        truth_map[cases[score]].append(i)\n        predictions_map[cases[preds]].append(i)\n\n    total_data = len(test_df)\n\n    print(\"Predictions\")\n    print(f'Negative: {len(predictions_map[cases[0]])} | Neutral: {len(predictions_map[cases[1]])} | Positive: {len(predictions_map[cases[2]])}')\n    print(\"============\\n\")\n    print(\"Truth\")\n    print(f'Negative: {len(truth_map[cases[0]])} | Neutral: {len(truth_map[cases[1]])} | Positive: {len(truth_map[cases[2]])}')\n    print(\"============\\n\")\n\n    tp_negative = len(set(predictions_map['negative']).intersection(set(truth_map['negative'])))\n    fp_negative = len(predictions_map['negative']) - tp_negative\n    fn_negative = len(truth_map['negative']) - tp_negative\n    tn_negative = total_data - (tp_negative + fp_negative + fn_negative)\n\n    tp_neutral = len(set(predictions_map['neutral']).intersection(set(truth_map['neutral'])))\n    fp_neutral = len(predictions_map['neutral']) - tp_neutral\n    fn_neutral = len(truth_map['neutral']) - tp_neutral\n    tn_neutral = total_data - (tp_neutral + fp_neutral + fn_neutral)\n\n    tp_positive = len(set(predictions_map['positive']).intersection(set(truth_map['positive'])))\n    fp_positive = len(predictions_map['positive']) - tp_positive\n    fn_positive = len(truth_map['positive']) - tp_positive\n    tn_positive = total_data - (tp_positive + fp_positive + fn_positive)\n\n    print(\"True Positives\")\n    print(f'Negative: {tp_negative} | Neutral: {tp_neutral} | Positive: {tp_positive}')\n    print(\"True Negatives\")\n    print(f'Negative: {tn_negative} | Neutral: {tn_neutral} | Positive: {tn_positive}')\n    print(\"False Negatives\")\n    print(f'Negative: {fn_negative} | Neutral: {fn_neutral} | Positive: {fn_positive}')\n    print(\"False Positives\")\n    print(f'Negative: {fp_negative} | Neutral: {fp_neutral} | Positive: {fp_positive}')\n\n    f1_negative = calculate_f1(tp_negative, fp_negative, fn_negative)\n    f1_neutral = calculate_f1(tp_neutral, fp_neutral, fn_neutral)\n    f1_positive = calculate_f1(tp_positive, fp_positive, fn_positive)\n    average_f1 = (f1_negative + f1_neutral + f1_positive) / 3\n\n    print(f\"F1 Score Negative: {f1_negative:.4f}\")\n    print(f\"F1 Score Neutral: {f1_neutral:.4f}\")\n    print(f\"F1 Score Positive: {f1_positive:.4f}\")\n    print(f\"Average F1 Score: {average_f1:.4f}\")\n    print(\"============\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')\ncompare(roberta_result.predictions, roberta_preds, roberta_model_type)\ncompare(trans_blstm_result.predictions, trans_blstm_preds, 'trans-blstm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare(bert_cnn_result.predictions, bert_cnn_preds,'bert-cnn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}